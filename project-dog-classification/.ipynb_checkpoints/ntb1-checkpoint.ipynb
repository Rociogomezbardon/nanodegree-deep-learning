{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 10%.  In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!\n",
    "\n",
    "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
    "\n",
    "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dog_images/train`, `dog_images/valid`, and `dog_images/test`, respectively).  You may find [this documentation on custom datasets](http://pytorch.org/docs/stable/torchvision/datasets.html) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#data_transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "training_data_transform = transforms.Compose([transforms.Resize(255),\n",
    "                transforms.CenterCrop(224), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "other_data_transform = transforms.Compose([transforms.CenterCrop(224), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "'''training_data_transform = transforms.Compose([transforms.Resize(255),\n",
    "                transforms.CenterCrop(224), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "other_data_transform = transforms.Compose([transforms.CenterCrop(224), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])'''\n",
    "\n",
    "train_data = datasets.ImageFolder(\"/data/dog_images/train\", transform = training_data_transform)\n",
    "test_data = datasets.ImageFolder(\"/data/dog_images/test\", transform = other_data_transform)\n",
    "valid_data = datasets.ImageFolder(\"/data/dog_images/valid\", transform = other_data_transform)\n",
    "\n",
    "# define dataloader parameters\n",
    "batch_size = 20\n",
    "num_workers = 0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "loaders_scratch = {\"train\" : train_loader, \"test\" : test_loader, \"valid\" : valid_loader}\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Describe your chosen procedure for preprocessing the data. \n",
    "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
    "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  Use the template in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # input = (3 x 224 x 224)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        # input = (16 x 112 x 112)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        #input = (32 x 56 x 56)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding =1)\n",
    "        #input = (64 x 28 x 28)\n",
    "        self.fc1 = nn.Linear(32*28*28, 500)\n",
    "        # input (500)\n",
    "        self.fc2 = nn.Linear(500,len(train_data.classes))\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x))) \n",
    "        x = x.view(-1, 32*28*28)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and the optimizer as `optimizer_scratch` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: specify optimizer\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.005 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_scratch.pt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the following import is required for training to be robust to truncated images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    last_valid_loss = 0.0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            output = torch.squeeze(output)\n",
    "            # calculate the batch loss\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "           \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < last_valid_loss: \n",
    "            torch.save(model, save_path)\n",
    "                \n",
    "        last_valid_loss = valid_loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "   \n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.888361 \tValidation Loss: 4.882190\n",
      "Epoch: 2 \tTraining Loss: 4.877177 \tValidation Loss: 4.868124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 4.859826 \tValidation Loss: 4.844392\n",
      "Epoch: 4 \tTraining Loss: 4.817468 \tValidation Loss: 4.782650\n",
      "Epoch: 5 \tTraining Loss: 4.714293 \tValidation Loss: 4.699317\n",
      "Epoch: 6 \tTraining Loss: 4.614477 \tValidation Loss: 4.644349\n",
      "Epoch: 7 \tTraining Loss: 4.505893 \tValidation Loss: 4.544355\n",
      "Epoch: 8 \tTraining Loss: 4.396420 \tValidation Loss: 4.549247\n",
      "Epoch: 9 \tTraining Loss: 4.316216 \tValidation Loss: 4.493648\n",
      "Epoch: 10 \tTraining Loss: 4.255379 \tValidation Loss: 4.474846\n",
      "Epoch: 11 \tTraining Loss: 4.201305 \tValidation Loss: 4.461491\n",
      "Epoch: 12 \tTraining Loss: 4.153007 \tValidation Loss: 4.458512\n",
      "Epoch: 13 \tTraining Loss: 4.101629 \tValidation Loss: 4.486062\n",
      "Epoch: 14 \tTraining Loss: 4.064032 \tValidation Loss: 4.481427\n",
      "Epoch: 15 \tTraining Loss: 4.017765 \tValidation Loss: 4.460796\n",
      "Epoch: 16 \tTraining Loss: 3.978249 \tValidation Loss: 4.483760\n",
      "Epoch: 17 \tTraining Loss: 3.931388 \tValidation Loss: 4.509509\n",
      "Epoch: 18 \tTraining Loss: 3.888627 \tValidation Loss: 4.505931\n",
      "Epoch: 19 \tTraining Loss: 3.843300 \tValidation Loss: 4.531826\n",
      "Epoch: 20 \tTraining Loss: 3.787609 \tValidation Loss: 4.514888\n"
     ]
    }
   ],
   "source": [
    "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch7.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.486484\n",
      "\n",
      "\n",
      "Test Accuracy:  6% (52/836)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6c4518f8d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.738961 \tValidation Loss: 4.541538\n",
      "Epoch: 2 \tTraining Loss: 3.694287 \tValidation Loss: 4.506902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 3.653617 \tValidation Loss: 4.580566\n",
      "Epoch: 4 \tTraining Loss: 3.600892 \tValidation Loss: 4.570092\n",
      "Epoch: 5 \tTraining Loss: 3.560436 \tValidation Loss: 4.600972\n",
      "Epoch: 6 \tTraining Loss: 3.511305 \tValidation Loss: 4.598420\n",
      "Epoch: 7 \tTraining Loss: 3.469071 \tValidation Loss: 4.604179\n",
      "Epoch: 8 \tTraining Loss: 3.406084 \tValidation Loss: 4.695367\n",
      "Epoch: 9 \tTraining Loss: 3.349212 \tValidation Loss: 4.725230\n",
      "Epoch: 10 \tTraining Loss: 3.289777 \tValidation Loss: 4.689101\n",
      "Epoch: 11 \tTraining Loss: 3.252340 \tValidation Loss: 4.790723\n",
      "Epoch: 12 \tTraining Loss: 3.183178 \tValidation Loss: 4.768089\n",
      "Epoch: 13 \tTraining Loss: 3.120008 \tValidation Loss: 4.748803\n",
      "Epoch: 14 \tTraining Loss: 3.091750 \tValidation Loss: 4.845723\n",
      "Epoch: 15 \tTraining Loss: 3.008573 \tValidation Loss: 4.875780\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-08fc4b470d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n\u001b[0;32m----> 3\u001b[0;31m                       criterion_scratch, use_cuda, 'model_scratch.pt')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b856d5b9e9b6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.886816\n",
      "\n",
      "\n",
      "Test Accuracy:  6% (55/836)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6c44f09d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c450428d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
    "\n",
    "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dogImages/train`, `dogImages/valid`, and `dogImages/test`, respectively). \n",
    "\n",
    "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Specify data loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Use transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "## TODO: Specify model architecture \n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/master/optim.html).  Save the chosen loss function as `criterion_transfer`, and the optimizer as `optimizer_transfer` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = None\n",
    "optimizer_transfer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model_transfer = # train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "#model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan hound`, etc) that is predicted by your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].classes]\n",
    "\n",
    "def predict_breed_transfer(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `human_detector` functions developed above.  You are __required__ to use your CNN from Step 4 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def run_app(img_path):\n",
    "    ## handle cases for a human face, dog, and neither\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that _you_ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Three possible points for improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "\n",
    "## suggested code, below\n",
    "for file in np.hstack((human_files[:3], dog_files[:3])):\n",
    "    run_app(file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
